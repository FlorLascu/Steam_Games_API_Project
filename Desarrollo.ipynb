{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "**Steam** is a digital video game distribution platform developed by Valve Corporation.<br>\n",
    "It was launched in September 2003 as a way for Valve to provide automatic updates to its games, but was eventually expanded to include third-party games. <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Starting out our ETL Process\n",
    "We started out by reading the Data Glossary to understand the structure and content of the three files provided with the project documentation: \n",
    "\n",
    "1.  **steam_games.json.gz**: This dataset contains comprehensive information about the games available on Steam. It encompasses details such as game titles, genres, release dates,etc... allowing us to gain insights into the **gaming catalog**. <br>\n",
    "\n",
    "2.  **user_reviews.json**: The user reviews dataset provides user-generated feedback for Steam games. It contains data such as review text and rating feedback votes, enabling us to analyze user sentiments and experiences effectively.<br>\n",
    "\n",
    "3.  **users_items.json**: In this dataset, we find data related to Steam users and their collections of in-game items. <br>\n",
    "The insightful data from this dataset includes user preferences and the specific items they own.\n",
    "\n",
    "Once we reviewed the structure, column definitions and set up, we proceeded to load them into Pandas Dataframes in order to access the information to transform and process.<br>\n",
    "<br>\n",
    "\n",
    "## 1.1 Games\n",
    "\n",
    "We proceeded with the [ETL_games](ETL_games.ipynb) : **steam_games.json.gz** without inconvenients:<br>\n",
    "\n",
    "The dataframe was named df_games, containing:<br>\n",
    "*   120.445 rows\n",
    "*   13 columns \n",
    "<br>\n",
    "Using Data Wrangler, we get  a general idea of the dataset, it contains many missing values, most columns have up to 70% missing values in the dataset.<br>\n",
    "\n",
    ">We will be deleting the missing information since, first we can't complete the missing information since it is diverse and the source is unknown.<br>\n",
    ">Also, deleting missing information still provides us with more than enough information to our project. There is no loss. \n",
    "\n",
    "Now we are left with 22.530 rows and 13 columns  of information <br>\n",
    "We now need to evaluate each column in order to decide which columns are important for the project and which columns can be deleted<br>\n",
    "<br>\n",
    "To follow, we will delete duplicates, only 2 registers were eliminated<br>\n",
    "\n",
    "#### Now, we can start to define which columns we do need to keep. <br>\n",
    "Columns that we already know that are necessary for the API functions: <br>\n",
    "\n",
    "*   **publisher** for function **def developer**\n",
    "*   **genres** for function **def user_for_genre**\n",
    "*   **release date** for function **def best_developer_year**\n",
    "*   **price** for function **def user_data**\n",
    "*   **titles** for **def best_developer_year**\n",
    "*   **id** identifies each game for its unique id value\n",
    "*   **developer** for function **def developer**\n",
    "##### These above columns, will be transformed and manipulated as part of the model.\n",
    "\n",
    "\n",
    "#### Also, we can start to evaluate the information from other columns in order to delete columns that we don't need from our dataset.<br>\n",
    "##### These are:\n",
    "*   **app_name** similar information as in title\n",
    "*   **url** does not bring value to the intended project\n",
    "*   **tags** similar to genres and does not bring value to the intended project\n",
    "*   **discount_price** does not bring value to the intended project\n",
    "*   **reviews_url** does not bring value to the intended project\n",
    "*   **specs** does not bring value to the intended project\n",
    "*   **early access** does not bring value to the intended project\n",
    "\n",
    "##### So all the above columns will be deleted   \n",
    "\n",
    "Adding value to our data:\n",
    "*   We convert **price** into an all float column, so all info about Free and promotions is turned into zeros.\n",
    "*   We create a new column = **release_year** and delete the previous column **release_year** this new column will be used in the function **best_developer_year**\n",
    "*   We drop all columns that don't add to the model or functions\n",
    "\n",
    "#### Now there is one last column that we need to analyze and transform: 'genres'\n",
    "##### genres\n",
    "This column contains a list in each of the rows indicating to which genres does the game belong to. <br>\n",
    "A **video game genre** is a category that groups games based on objectives, story, and gameplay.<br>\n",
    "It helps users and developers identify games with similar styles and themes, like the intense challenges in action games or character development and quests in role-playing games.\n",
    "In this column, for each game (row) we find a list of genres that the game features.<br>\n",
    "This information will be used  in the function **user_for_genre** and our recommendation model<br>\n",
    "But we will do this in our EDA, for now we will continue with our ETL stages.<br>\n",
    "\n",
    "We store the transformed dataframe in [games.parquet](games.parquet) in data. \n",
    "\n",
    "## 1.2 User_Reviews\n",
    "When moving on to our second file [ETL_user_reviews](ETL_user_reviews.ipynb), we were faced with our first challenge:<br>\n",
    "The standard proceedure to read the file used in the first file resulted in error after error, because the data inside the .json file was not in the expected format.<br>\n",
    "After investigating I learned that when the JSON file you are trying to load is not properly formatted (which is the case here), you can evaluate it line by line using **ast.literal_eval()**. <br>\n",
    "\n",
    "After extensive searching and delving into Stack Overflow, I stumbled upon an article that was a breakthrough:\n",
    "\n",
    "[Convert JSON to pd.DataFrame](https://stackoverflow.com/questions/55338899/convert-json-to-pd-dataframe/65427497#65427497)<br>\n",
    "[JSONDecodeError](https://bobbyhadz.com/blog/python-jsondecodeerror-expecting-property-name-enclosed-in-double-quotes)<br>\n",
    "\n",
    "The df_reviews dataframe (**user_reviews.json.gz**), contains:<br>\n",
    "*   25.485 rows\n",
    "*   3 columns\n",
    "<br>\n",
    "First we proceed to identify missing or duplicated values.\n",
    "We encountered 623 duplicated registers, we validate that they are in fact duplicated and we finally delete the duplicated value keeping just the first occurence of the value.<br>\n",
    "Then we move forward into the reviews column which is of the most interest to our project.<br>\n",
    " \n",
    "It was easy to detect that the third column was a collection of list of dictionaries, and that data inside the last column is vital for the development of the sentiment analysis.<br>\n",
    "```\n",
    "df_reviews.iloc[100,2]\n",
    "```\n",
    "[{'funny': '',\n",
    "  'posted': 'Posted October 13, 2014.',\n",
    "  'last_edited': '',\n",
    "  'item_id': '209870',\n",
    "  'helpful': '3 of 8 people (38%) found this review helpful',\n",
    "  'recommend': True,\n",
    "  'review': 'Its a very fun game i recomend as its nearly like TITANFALL but its FREE!Play this game now'}]\n",
    "  \n",
    "#### We need to extract into columns the information present inside each field from the third column. <br> \n",
    "The fields into columns will be: funny, posted, last_edited,item_id,helpful,recommend, review.\n",
    "After several attempts of different approaches I found a page with an explanation to a similar exercise that I adapted to resolve the nested list of dicctionaries into columns and registers combination.<br>\n",
    "[Convert list of nested dictionary into Pandas dataframe](https://www.geeksforgeeks.org/python-convert-list-of-nested-dictionary-into-pandas-dataframe/)\n",
    "\n",
    "Finally, the unnested df contains:\n",
    "*   59.305 rows \n",
    "*   9 columns (['user_id', 'user_url', 'funny', 'posted', 'last_edited', 'item_id',\n",
    "       'helpful', 'recommend', 'review_text']) \n",
    "So the final result is a dataframe with 25.485 rows, zero missing values, and columns:\n",
    "\n",
    "* **user_id**: unique id identifier for each user in the platform\n",
    "* **user_url**: url hosting the user profile in streamcommunity.\n",
    "* **reviews**: contains a list of dictionaries. For each user, listing all the reviews posted by the user:\n",
    "    * **funny**: indicates if another user qualifies as funny the users review\n",
    "    * **posted**: date when the posting was made, format:  Posted April 21, 2011.\n",
    "    * **last_edited**: date when the post was lastedited  \n",
    "    * **item_id**: Is the game id, unique for each game\n",
    "    * **helpful**: other users qualify if the review was render usefull\n",
    "    * **recommend**: if a boolean value that indicates whether the user recommends the games or not.\n",
    "    * **review**: Is the written review from the user that is post to the game url.\n",
    "\n",
    "From all of these columns we found two columns with mostly missing values, missing values of the format '' where then changed to None. <br>\n",
    "And after reviewing the amount of None values in all added columns, we decided to drop columns:\n",
    "- **funny** with more than 86% of None values\n",
    "- **last_edited** with more than 89% of None values\n",
    "\n",
    "Then we proceeded to transform the column 'posted' from a 'Posted {Month} {day}, {year}' format into a new 'date' column.\n",
    "In this column we found 9929 registers with the missing year, this were completed with the current year (2024) in order to be differentiate from the rest of the correct extracted values.\n",
    "This 2024 year registers will not be considered for the **best_developer_year** function, but the remaining information still is available for the model.<br>\n",
    "\n",
    "The remaining dataframe [reviews.parquet](reviews.parquet) summarizes 58.400 rows and 9 columns<br>\n",
    "\n",
    "## 1.2 User_Items\n",
    "Finaly, we moved on to our third and last file [ETL_items](ETL_items.ipynb): **users_items.json.gz**\n",
    "We get the same error as the second file, so we need to load the information using the literal function from ast.<br>\n",
    "\n",
    "We find yet again another column, **items** with nested list of dictionaries, and the information found in this column is the list of items (games) that each user purchased. So this is all valueable information that we need to develop our functions for our model.<br>\n",
    "Before moving into unnesting the 'items' column we check once again for duplicates using the 'user_id' field.<br>\n",
    "We found 1357 duplicated registers, that after validating that they were duplicated we proceeded to drop them keep the first occurrence.<br>\n",
    "\n",
    "Finally, we proceed as with the previous reviews file, learning what is the structure of the data we need to extract from the **'items'** column<br>\n",
    "\n",
    "The columns to add into our df_items are:\n",
    "```\n",
    "df_items.iloc[10,4]\n",
    "```\n",
    "[{'item_id': '4000',\n",
    "  'item_name': \"Garry's Mod\",\n",
    "  'playtime_forever': 2644,\n",
    "  'playtime_2weeks': 0},\n",
    " {'item_id': '1250',\n",
    "  'item_name': 'Killing Floor',\n",
    "  'playtime_forever': 30266,\n",
    "  'playtime_2weeks': 0},\n",
    " {'item_id': '35420',\n",
    "  'item_name': 'Killing Floor Mod: Defence Alliance 2',\n",
    "  'playtime_forever': 54,\n",
    "  'playtime_2weeks': 0},\n",
    "\n",
    "...\n",
    "  'playtime_2weeks': 462},\n",
    " {'item_id': '444640',\n",
    "  'item_name': 'Bloons TD Battles',\n",
    "  'playtime_forever': 809,\n",
    "  'playtime_2weeks': 0}]\n",
    "\n",
    "  So we need to iterate through the item field for each row, in order to extract for each user the information:<br>\n",
    "  - **'item_id'**: '444640' --> The unique identifier for the game, **game_id**<br>\n",
    "  - **'item_name'**: 'Bloons TD Battles',--> the **Name** for the game<br>\n",
    "  - **'playtime_forever'**: 809,--> the amount of total time the user played the game in **minutes**<br>\n",
    "  - **'playtime_2weeks'**: 0}] --> the amount of total time the user played the game for the **last two weeks**<br>\n",
    "\n",
    "So we run our code to convert for each user, this information into columns.<br>\n",
    "<br>\n",
    "We then obtain a dataframe of:<br>\n",
    "- 5.153.209 rows<br>\n",
    "- 8 columns<br>\n",
    "<br>\n",
    "Now we proceed to get more insight into the different columns and data in order to decide which column is relevant to our project and which column can be drop.<br>\n",
    "\n",
    "#### Valuable information and disposable data\n",
    "Now that we have some insight into the dataframe structure and information we can conclude that there are many columns we can drop in order to keep our data manageable and as compact as possible<br>\n",
    "##### Columns to drop:<br>\n",
    "-   **items_count** is a column that given the information can be calculated by request<br>\n",
    "-   **steam_id** is the same exact value as **user_id**\n",
    "-   **user_url** it is already contained in the review dataframe so in case it is needed it can be then pulled from that dataframe by request<br>\n",
    "<br>\n",
    "\n",
    "The remaining dataframe [items.parquet](items.parquet) summarizes 5.094.082 rows and 5 columns<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Step 2 - EDA Analysis Summary\n",
    "Now that we have the three files clean and ready, we can start getting some insight  into the data that will be used in our recommendation model project.<br>\n",
    "Here you will find a summary of the work done in the EDA Analysis. <br>\n",
    "For more detail information and visualizations:<br>\n",
    "\n",
    "[EDA Games](EDA_games.ipynb)\n",
    "\n",
    "[EDA Reviews](EDA_reviews.ipynb)\n",
    "\n",
    "[EDA Items](EDA_items.ipynb)\n",
    "\n",
    "## 2.1 Games\n",
    "\n",
    "In the **EDA Analysis** we will mostly focus on:\n",
    "-   search for outlier values\n",
    "-   data statistics and description\n",
    "-   data transformation\n",
    "-   Variables:\n",
    "    - item_id: this column contains the unique value identifier for the game.\n",
    "    - item_name: this is the game name\n",
    "    - developer: is a software developer specializing in video game development â€“ the process and related disciplines of creating video games.\n",
    "    - pucblisher: A video game publisher is a company that publishes video games that have been developed either internally by the publisher or externally by a video game developer\n",
    "    - release_year: is the year the game was released\n",
    "    - genres: this column contains a list of genres for each game, we will be transforming this data with different approaches<br>\n",
    "\n",
    "### A summary of what you will find in the notebook:\n",
    "#### Price\n",
    "    *   Find out if our price information has outliers\n",
    "    *   Most expensive games\n",
    "    *   Average game price\n",
    "    *   Price Range\n",
    "    *   Price Distribution\n",
    "    *   Price Statistics\n",
    " \t\n",
    "#### Relesease Date\n",
    "    *   Year range\n",
    "    *   Games released per year\n",
    "    *\tTop five years with more released games\n",
    "\n",
    "#### Publishers\n",
    "    *   Top ten publishers, largest amount of published games \n",
    "\n",
    "#### Developers\n",
    "    *   Top ten developers, the developers with the largest amount of games published\n",
    "    *   Least popular or relevant developers, developers with the smallest amount of published games    *   Least important publishers, publishers with the smallest amount of published games \n",
    "\n",
    "#### Genres\n",
    "    *   Most popular genres, genres with the largest amount of published games\n",
    "    *   Least popular genres, genres with the largest amount of published games\n",
    "\n",
    "### Correlation Analysis between genres and price\n",
    "##### Has the genres impact on the price of the games? \n",
    "\n",
    "## 2.2 Reviews\n",
    "\n",
    "**The final purpose of this analysis is to define, for the recommendation model requested and the different functions, for each dataset and function the corresponding columns and data that will be requiered** \n",
    "\n",
    "### During the ETL process we already took care of:\n",
    "\n",
    "[ETL_user_reviews](ETL_user_reviews.ipynb)<br>\n",
    "\n",
    "-   missing values and data cleaning\n",
    "-   removed columns of irrelevant information \n",
    "-   removed rows with non existant information or not valid.\n",
    "-   removed duplicated values\n",
    "-   corrected all datatypes for each and every data and column\n",
    "\n",
    "### Sentiment Analysis\n",
    "The dataset we will be using has already underwent the sentiment analysis transformation, so intead of working the EDA with reviews we will have a column named 'sentiment' that shows the result of the Sentiment Analyis process.<br>\n",
    "\n",
    "### So in the **EDA Analysis** we will mostly focus on:\n",
    "-   search for outlier values\n",
    "-   data statistics and description\n",
    "-   data transformation\n",
    "-   Variables:\n",
    "    - user_id: this column contains the unique value identifier for each of the users in Steam.\n",
    "    - user_url: this column contains the url to the profile for each of the users in Steam.\n",
    "    - item_id: this column contains the unique value identifier for the game.   \n",
    "    - helpful: In this column other users qualify if the review was render usefull<br>\n",
    "    - recommend: This column is a boolean value that indicates whether the user recommends the games or not. \n",
    "    - date: is the date when review was posted. From this column we extract the column year\n",
    "    - year: is the year the game was released\n",
    "    - sentiment: this column contains the sentiment analysis classification value<br>\n",
    "        0 --> Negative\n",
    "        1 --> Neutral\n",
    "        2 --> Positive\n",
    "\n",
    "### A summary of what you will find in the notebook:\n",
    "#### user_id\n",
    "-   Total number of users on the platform.\n",
    "-   Calculation of the average number of reviews per user.\n",
    "-   Exploration of the distribution of the number of reviews per user.\n",
    "-   Determination of the user with the highest number of reviews.\n",
    "-   Identification of the user with the highest average number of reviews per year.\n",
    "\n",
    "**Findings:**\n",
    "\n",
    "*   Only 30% of the users in the Steam platform post reviews, this is 25.446 users\n",
    "*   this 30% of the users post in average 2.3 reviews \n",
    "*   from this 30%, 53% of users post just one review\n",
    "*   And Less than 2% from the users that post, post more than 8 reviews\n",
    "\n",
    "#### item_id\n",
    "-\tHow many games have reviews\n",
    "-\tWhat percentage of games have reviews from the total amount of games available in the platform\n",
    "-   \tExploration of the distribution of the number of reviews per game\n",
    "-\tAverage reviews per game\n",
    "-\tIdentification of the top 10 games with the highest number of reviews\n",
    "-\tGames with more than 500 reviews\n",
    "\n",
    "**Findings:**\n",
    "\n",
    "*\t16% of all registered games in the platform get reviews, 3.682 out of a total of 22.530.\n",
    "*\t15% of the games get between 1 and 12 reviews\n",
    "*\tThe average review per game is 15 reviews\t\n",
    "*\t67% of all the reviews are concentrated in just 12 games.\n",
    "\n",
    "### Helpful\n",
    "In this column other users qualify if the review was render usefull<br>\n",
    "This is how many users have qualify the posted review useful.<br>\n",
    "We are going to transform this column in order to get some insight on how other users render their rating of the user's review\n",
    "\n",
    "**Findings:**\n",
    "* Most reviews are not rated by other users, **61% of them are not rated**\n",
    "* 17% of the rated reviews are render helpful by other users\n",
    "\n",
    "## Recommend and Sentiment Analysis\n",
    "**We conducted a bivariate analyisis**<br>\n",
    "\n",
    "In the case of this two variables we are going to run the EDA as a bivariable analysis.<br>\n",
    "This column is a boolean value that indicates whether the user recommends the games or not. This is if the recomendation is positive there is a 1 and if not a zero.<br>\n",
    "We can use this column to evaluate or validate our sentiment analysis result. <br>\n",
    "- How many users post a positive recommendation<br>\n",
    "- How many users post a positive recommendation and a positive sentiment review<br>\n",
    "- How many positive recommendations match to a positive sentiment<br>\n",
    "- How many negative sentiment match to a negative recommendation<br>\n",
    "\n",
    "#### Sentiment Analysis and Recommend\n",
    "We are going to consider that all neutral sentiments are positive, and we are going to conduct a type of accuracy score to the sentiment analysis using NLKT and TextBlob libraries given the recommend column.<br>\n",
    "This is to analyze in some way how accurate our sentiment analysis is.<br>\n",
    "\n",
    "**Findings**\n",
    "From the accuracy evaluation between both sentiment Analysis we conclude that the NLTK analysis is the best performing.<br>\n",
    "Now we can review for the results against the recommend from the user:<br>\n",
    "\n",
    "* **Negatives** We can see that for the recommend True, this is that the user recommends the game even if the review is negative, the percentage of recommend is about 70%, this is that most users although they consider the game not good, they still recommended the game.<br>\n",
    "* **Neutral** In this case, the review being neutral is not a negative review, so it is not a surprise that users still recommend the game even if the review is not bad. And also, our sentiment analysis performs very good recognizing positive reviews but not so good in classifying neutral reviews.<br>\n",
    "* **Positive** In this case, we confirm what our accuracy test showed, our sentiment analysis is good at classifying positive reviews, and positive reviews match on 91% of the cases.\n",
    "Given that our analysis was a fast and simple approach with many opportunities, the results are good, all things considered<br>\n",
    "\n",
    "\n",
    "## 2.3 Items\n",
    "### During the ETL process we already took care of:\n",
    "\n",
    "[ETL_items](ETL_items.ipynb)<br>\n",
    "\n",
    "-   missing values and data cleaning\n",
    "-   removed columns of irrelevant information \n",
    "-   removed rows with non existant information or not valid.\n",
    "-   removed duplicated values\n",
    "-   corrected all datatypes for each and every data and column\n",
    "\n",
    "### So in the **EDA Analysis** we will mostly focus on:\n",
    "-   search for outlier values\n",
    "-   data statistics and description\n",
    "-   data transformation\n",
    "-   Variables:\n",
    "    - user_id: This user_id is the unique identifier for each of the users in the platform column \n",
    "    - item_id: unique identifier for the game in the platform\n",
    "    - item_name: name of the game or app\n",
    "    - playtime_forever: The total number of minutes played \"on record\"\n",
    "    - playtime_2weeks: The total number of minutes played on the last 2 weeks\n",
    "-   Multivariable analysis:\n",
    "\n",
    "### A summary of what you will find in the notebook:\n",
    "\n",
    "#### User_id\n",
    "This user_id is the unique identifier for each of the users in the platform column\n",
    "**Findings**\n",
    "- How many users are registered in the platform\n",
    "\n",
    "#### item_id\n",
    "- How many games are items in the users accounts\n",
    "- Percentage from the total amount of games\n",
    "- Average amount of games per user\n",
    "- Top 10 items most sold (most users have it)\n",
    "- Top 10 Games most played in the last 2 weeks\n",
    "\n",
    "#### Outliers for items_id per user\n",
    "\n",
    "**Findings**\n",
    "* The average amount of games per user are 71 games\n",
    "* Over 50% of the users own more than 40 games each.\n",
    "* There are only 39 users with more than 2000 games each, this will be considered outliers but they represent less than the 0.056%\n",
    "* The **maximum amount** of games owned by a user is *7762 games*\n",
    "- Top 10 items most sold (most users have it)\n",
    "\n",
    "#### item_name\n",
    "This is the name of the game.\n",
    "\n",
    "### playtime_forever\n",
    "The total number of minutes played \"on record\"\n",
    "- \tTop 5 users with the most minutes played in total\n",
    "-\tTop 5 games with the most minutes played\n",
    "-\tAverage number of minutes played by user\n",
    "-\tAverage number of minutes per game\n",
    "\n",
    "**Findings**\n",
    "* The Top 10 games account for about 41% of the total played time.\n",
    "* The average playtime per user is 1497 minutes\n",
    "* 50% of all users play more than 887 minutes.\n",
    "* The average minutes per games is 277 minutes\n",
    "* 50% of the games are played over more than 90 minutes in average\n",
    "\n",
    "### playtime_2weeks\n",
    "The total number of minutes played \"on record\" for the last 2 weeks\n",
    "- \tTop 5 users with the most minutes played in total\n",
    "-\tTop 5 games with the most minutes played\n",
    "-\tAverage number of minutes played by user\n",
    "-\tAverage number of minutes per game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "\n",
    "## 3.1 Sentiment Analysis\n",
    "To gain some insight and true value from our user's reviews we will conduct a Sentiment Analysis from the reviews of our users.\n",
    "This **Sentiment Analysis** will then be used in our recommendation system, model.<br>\n",
    "For this, we chose to work with NLTK pyhton libraries <br>\n",
    "There is not a unique approach to Sentiment Analysis, all methods can deliver a good result. \n",
    "I our project we chose NLTK library and not TextBlob that is a much simpler approach:\n",
    "\n",
    "NLTK (Natural Language Toolkit):\n",
    "\n",
    "**Advantages**:\n",
    "-   Provides a wide range of tools and resources for natural language processing tasks.\n",
    "-   Flexible and customizable for advanced NLP tasks.\n",
    "-   Well-established community and extensive documentation.\n",
    "\n",
    "**Disadvantages**:\n",
    "-   Requires more code and effort for simple tasks compared to TextBlob.\n",
    "-   May be slower for certain tasks due to its comprehensive nature.\n",
    "\n",
    "In our case we studied and implemented both methods, you can review in detail in [Feature Engineering](03.%20Feature%20Engineering.ipynb).\n",
    "\n",
    "We proceeded following the process of:\n",
    "\n",
    "1. **Data Preprocessing and Tokenization**: This step involves cleaning and preparing the text data for analysis. It includes tasks such as removing punctuation, converting text to lowercase, removing stop words, and handling special characters.\n",
    "Tokenization involves breaking down the text into individual words or tokens.\n",
    "> We chose to perform this by creating a new column, named 'clean_review' and passing a function to the 'review_text' column  content and storing this transformation, for each element in the 'clean_review' \n",
    "\n",
    "2. **Sentiment Analysis**: Once the text data is prepared, sentiment analysis can be performed. There are several libraries and techniques available for sentiment analysis, including lexicon-based approaches, machine learning models, and deep learning models.\n",
    "We set to define the sentiment classification into:\n",
    "*   Negative = 0 for all values where compound_score was under - 0.05\n",
    "*   Neutral = 1 all other results nor negative nor positive and none values or emojis.\n",
    "*   Positive = 2 for all values where compound_score was above 0.05\n",
    "> For our work, we chose to work with Vader. ALthough Textblob is a suitable solution because is much faster and simpler, when comparing both results, the only disadvanage of NLTK was the use of resources, the model runs slow, but the outcome is better.\n",
    ">NLTK is more assertive in classifing negative and positive sentiments, but even better for neurtral classification.\n",
    "In the [Feature Engineering](03.%20Feature%20Engineering.ipynb) you can review in detail the process for both approaches and our analysis, and then in the [EDA_reviews](EDA_reviews.ipynb) you can find the comparison of the results for both approaches and more exploration.<br>\n",
    "\n",
    "3. **Analysis, Visualization and Summary**: Finally, the results of the sentiment analysis can be analyzed and visualized to gain insights from the data.\n",
    "\n",
    "* Resources:<br>\n",
    "[Sentiment Analysis](https://www.youtube.com/watch?v=O_B7XLfx0ic)\n",
    "\n",
    "[NLTK Sentiment Analysis](https://www.youtube.com/watch?v=XFoehWRzG-I)\n",
    "\n",
    "[Sentiment_Analysis_article](https://www.kdnuggets.com/sentiment-analysis-in-python-going-beyond-bag-of-words)\n",
    "\n",
    "## 3.2  Rating our users reviews in the reviews dataframe.\n",
    "## Reviews Transformation\n",
    "### We need to transform our users reviews, recommend button result and the rating from helpful (where other users qualified the review (rate) into a rating) in order to feed our recommendation system.<br>\n",
    "For this we create a function that will consider all three items, when existing and average a rating that expresses the user's review in numbers.<br>\n",
    "Overall, this function assigns a rating score based on the provided values for 'rate', 'sentiment', and 'recommend', considering different combinations of these values and following specific rules.<br>\n",
    "*   If the 'rate' column is greater than 0:\n",
    "        -If the 'recommend' column is 0:\n",
    "            -If the 'sentiment' column is 0, the rating score is set to 0.\n",
    "            -If the 'sentiment' column is 1, the rating score is calculated as 50 times the 'rate'.\n",
    "            -If the 'sentiment' column is 2, the rating score is calculated as 75 times the 'rate'.\n",
    "        -If the 'recommend' column is 1:\n",
    "            -If the 'sentiment' column is 0, the rating score is calculated as 50 times the 'rate'.\n",
    "            -If the 'sentiment' column is 1, the rating score is calculated as 75 times the 'rate'.\n",
    "            -If the 'sentiment' column is 2, the rating score is calculated as 100 times the 'rate'.\n",
    "*   If the 'rate' column is 0:\n",
    "    - If the 'recommend' column is 0:\n",
    "        - If the 'sentiment' column is 0, the rating score is set to 0.\n",
    "        - If the 'sentiment' column is 1, the rating score is set to 50.\n",
    "        - If the 'sentiment' column is 2, the rating score is set to 75.\n",
    "    - If the 'recommend' column is 1:\n",
    "        - If the 'sentiment' column is 0, the rating score is set to 50.\n",
    "        - If the 'sentiment' column is 1, the rating score is set to 75.\n",
    "        - If the 'sentiment' column is 2, the rating score is set to 100\n",
    "Then we normalize this rating into a 1 to 5 score.<br>\n",
    "\n",
    "This rating will be the engine for our Collaborative Filtering Recommender System and the Machine Learning KNN Recommender <br>\n",
    "\n",
    "## 3.3 Collaborative Filtering Recommender\n",
    "\n",
    "## 3.4 Machine Learning KNN Recommender\n",
    "\n",
    "## 3.5 API Functions \n",
    "In this section of our notebook we are going to experiment and work on our requested API Functions.<br>\n",
    "These are:<br>\n",
    "1. **Developer** number of games and percentage of Free content by developer by year.<br>\n",
    "2. **User_data**: how much money has the user spent, what percentage, from the total number of games the user owns, has the user recommended from the reviews.recommend and how many games he purchased.<br>\n",
    "3. **User_for_genres**: this function must return the user with the largest amount of minutes for the given genres and a list of minutes accumulated per year since the release date<br>\n",
    "4. **Best_developer_year**: this function returns the top 3 developers based on the largest amount of recommendations for the given year (reviews.recommend = True and sentiment = 2)<br>\n",
    "5. **Developer_Reviews_Analyis**: given a developer, the function returns a dicctionary with developer as keys and the amount of Positive and Negative Sentiment Reviews.<br> \n",
    "\n",
    "In the notebook [Feature Engineering](03.%20Feature%20Engineering.ipynb) is the complete step by step to create each of this functions, and their input dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deploy an API in Render](https://medium.com/@iamgreatdiro/deploying-apis-on-render-a-step-by-step-guide-4ebe6a3fd377)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
